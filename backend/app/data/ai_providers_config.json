{
  "openai": {
    "name": "openai",
    "base_url": "https://api.openai.com/v1",
    "default_model": "gpt-4",
    "models": ["gpt-4", "gpt-3.5-turbo", "gpt-4-turbo"],
    "priority": 1,
    "supports_streaming": true,
    "max_tokens": 4096,
    "temperature_range": [0.0, 2.0]
  },
  "claude": {
    "name": "claude",
    "base_url": "https://api.anthropic.com",
    "default_model": "claude-3-sonnet-20240229",
    "models": ["claude-3-sonnet-20240229", "claude-3-haiku-20240307", "claude-3-opus-20240229"],
    "priority": 2,
    "supports_streaming": true,
    "max_tokens": 4096,
    "temperature_range": [0.0, 1.0]
  },
  "mistral": {
    "name": "mistral",
    "base_url": "https://api.mistral.ai",
    "default_model": "mistral-large-latest",
    "models": ["mistral-large-latest", "mistral-medium-latest", "mistral-small-latest"],
    "priority": 3,
    "supports_streaming": true,
    "max_tokens": 4096,
    "temperature_range": [0.0, 1.0]
  },
  "ollama": {
    "name": "ollama",
    "base_url": "http://localhost:11434",
    "default_model": "llama2",
    "models": ["llama2", "codellama", "mistral", "llama2:13b", "llama2:7b"],
    "priority": 4,
    "supports_streaming": true,
    "max_tokens": 4096,
    "temperature_range": [0.0, 2.0],
    "is_local": true
  },
  "gemini": {
    "name": "gemini",
    "base_url": "https://generativelanguage.googleapis.com",
    "default_model": "gemini-pro",
    "models": ["gemini-pro", "gemini-pro-vision"],
    "priority": 5,
    "supports_streaming": true,
    "max_tokens": 8192,
    "temperature_range": [0.0, 1.0]
  }
} 